{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Load the dataset\ndf = pd.read_csv('your_data.csv')\n\n# Display dataset shape and first few rows\nprint(f\"Dataset Shape: {df.shape}\")\nprint(f\"First 5 Rows:\\n{df.head()}\")\n\n# Separate features and target variable\nX = df.drop(columns=['target_column'], axis=1)\ny = df['target_column']\n\n# Identify categorical and numerical features\nnumeric_features = X.select_dtypes(include=[np.number]).columns\ncategorical_features = X.select_dtypes(include=[np.object_]).columns\n\n# Define preprocessing for numerical and categorical features\nnum_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, num_features),\n        ('cat', cat_transformer, cat_features)\n    ]\n)\n\n# Define multiple models for comparison\nmodels = {\n    'LogisticRegression': Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', LogisticRegression(max_iter=1000))\n    ]),\n    'KNeighborsClassifier': Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', KNeighborsClassifier())\n    ]),\n    'DecisionTreeClassifier': Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('model', DecisionTreeClassifier(random_state=42))\n    ])\n}\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Evaluate models using cross-validation\nprint(\"Evaluating models using cross-validation...\\n\")\nmodel_scores = {}\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor model_name, model_pipeline in models.items():\n    cv_scores = cross_val_score(model_pipeline, X_train, y_train, cv=kf, scoring='accuracy', n_jobs=-1)\n    model_scores[model_name] = np.mean(cv_scores)\n    print(f\"{model_name}: Average Accuracy = {np.mean(cv_scores):.4f}\")\n    print(f\"{model_name}: Standard Deviation of Accuracy = {cv_scores.std():.4f}\")\n    print(f\"{model_name}: 95% Confidence Interval (2.5th and 97.5th Percentiles) = {np.quantile(cv_scores, [0.025, 0.975]):.4f}\")\n\n# Select the best model based on CV scores\nbest_model_name = max(model_scores, key=model_scores.get)\nprint(\"\\n\" + \"=\"*150)\nprint(f\"Best model based on cross-validation scores: {best_model_name}\")\nprint(\"=\"*150)\n\n# Define hyperparameter grid for the selected model\nif best_model_name == 'KNeighborsClassifier':\n    param_grid = {\n        'model__n_neighbors': [3, 5, 7, 9],\n        'model__weights': ['uniform', 'distance'],\n        'model__metric': ['euclidean', 'manhattan']\n    }\nelif best_model_name == 'LogisticRegression':\n    param_grid = {\n        'model__C': [0.1, 1, 10],\n        'model__solver': ['liblinear', 'lbfgs']\n    }\nelif best_model_name == 'DecisionTreeClassifier':\n    param_grid = {\n        'model__max_depth': [None, 10, 20, 30],\n        'model__min_samples_split': [2, 5, 10],\n        'model__min_samples_leaf': [1, 2, 4]\n    }\n\n# Perform grid search on the best model\nprint(f\"\\nPerforming grid search for {best_model_name}...\")\nbest_pipeline = models[best_model_name]\ngrid_search = GridSearchCV(best_pipeline, param_grid, cv=kf, scoring='accuracy', verbose=2, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Display the best hyperparameters and CV score\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Best parameters for {best_model_name}: {grid_search.best_params_}\")\nprint(f\"Best cross-validated Accuracy: {grid_search.best_score_:.4f}\")\nprint(\"=\"*50)\n\n# Evaluate the best model on the test set\nfinal_model = grid_search.best_estimator_\ny_pred = final_model.predict(X_test)\n\nprint(\"\\n\" + \"=\"*150)\nprint(f\"Test Accuracy for the best model ({best_model_name}): {final_model.score(X_test, y_test):.4f}\")\nprint(\"=\"*150)\n\n# Display confusion matrix and classification report\nprint(\"\\nConfusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Completion message\nprint(\"\\nPipeline training, hyperparameter tuning, and testing completed successfully!\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}