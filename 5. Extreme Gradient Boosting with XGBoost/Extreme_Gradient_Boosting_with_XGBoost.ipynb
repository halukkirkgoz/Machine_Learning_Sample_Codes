{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -q -y scikit-learn\n",
        "!pip install -q scikit-learn==1.4.0"
      ],
      "metadata": {
        "id": "gdZUWCn7z3gh"
      },
      "id": "gdZUWCn7z3gh",
      "execution_count": 1,
      "outputs": []
    },
    {
      "id": "df0360f7-ed4c-4e3f-b1d5-1fbd464b3449",
      "cell_type": "code",
      "source": [
        "#========================= XGBOOST CLASSIFIER =========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "# Loading the breast cancer dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "X = data.data        # Features\n",
        "y = data.target      # Target labels\n",
        "\n",
        "# Splitting the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=123\n",
        ")\n",
        "\n",
        "# Initializing the XGBoost classifier\n",
        "xg_cl = xgb.XGBClassifier(\n",
        "    objective='binary:logistic', # Binary classification task\n",
        "    n_estimators=10,             # Number of boosting rounds (trees)\n",
        "    seed=123                     # Seed for reproducibility\n",
        ")\n",
        "\n",
        "# Training the classifier on the training data\n",
        "xg_cl.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the class labels for the test set\n",
        "preds = xg_cl.predict(X_test)\n",
        "\n",
        "# Calculating and displaying accuracy\n",
        "accuracy = accuracy_score(y_test, preds)\n",
        "\n",
        "# Displaying results in a clean and formatted manner\n",
        "print(\"=\"*30 + \" RESULTS \" + \"=\"*30)\n",
        "print(f\"Dataset: Breast Cancer (from sklearn)\")\n",
        "print(f\"Accuracy: {accuracy:.3%}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, preds, target_names=data.target_names))\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df0360f7-ed4c-4e3f-b1d5-1fbd464b3449",
        "outputId": "d83d91bc-d635-42ea-ccf3-f07e80ec2a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================== RESULTS ==============================\n",
            "Dataset: Breast Cancer (from sklearn)\n",
            "Accuracy: 98.246%\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.98      0.98      0.98        41\n",
            "      benign       0.99      0.99      0.99        73\n",
            "\n",
            "    accuracy                           0.98       114\n",
            "   macro avg       0.98      0.98      0.98       114\n",
            "weighted avg       0.98      0.98      0.98       114\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "id": "7e26d34e-f9aa-46da-a86f-f0ce59cbcaa6",
      "cell_type": "code",
      "source": [
        "#========================= XGBOOST CLASSIFIER CROSS-VALIDATION =========================\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "# Loading the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)  # Features as DataFrame\n",
        "y = pd.Series(data.target, name=\"target\")                # Target as Series\n",
        "\n",
        "# Splitting into training and testing sets (though only training is needed for CV)\n",
        "X_train, _, y_train, _ = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# Converting the training data into an XGBoost DMatrix\n",
        "churn_dmatrix = xgb.DMatrix(data=X_train, label=y_train)\n",
        "\n",
        "# Setting parameters for the XGBoost model\n",
        "params = {\n",
        "    \"objective\": \"binary:logistic\",  # Binary classification\n",
        "    \"max_depth\": 4                  # Maximum depth of the trees\n",
        "}\n",
        "\n",
        "# Performing cross-validation with 4 folds\n",
        "cv_results = xgb.cv(\n",
        "    dtrain=churn_dmatrix,           # Data for training\n",
        "    params=params,                  # Model parameters\n",
        "    nfold=4,                        # Number of cross-validation folds\n",
        "    num_boost_round=10,             # Number of boosting rounds\n",
        "    metrics=\"error\",                # Evaluation metric\n",
        "    as_pandas=True,                 # Return results as a DataFrame\n",
        "    seed=123                        # Seed for reproducibility\n",
        ")\n",
        "\n",
        "# Calculating accuracy from cross-validation results\n",
        "final_accuracy = (1 - cv_results[\"test-error-mean\"]).iloc[-1]\n",
        "\n",
        "# Displaying the results\n",
        "print(\"=\"*30 + \" RESULTS \" + \"=\"*30)\n",
        "print(f\"Dataset: Breast Cancer (from sklearn)\")\n",
        "print(f\"Final Cross-Validated Accuracy: {final_accuracy:.2%}\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e26d34e-f9aa-46da-a86f-f0ce59cbcaa6",
        "outputId": "eb25e650-48a2-49b4-afa1-4c18bd1087f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================== RESULTS ==============================\n",
            "Dataset: Breast Cancer (from sklearn)\n",
            "Final Cross-Validated Accuracy: 95.82%\n",
            "======================================================================\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "id": "10e3f4b9-4cf2-4a94-9177-1e3b0b92f0fe",
      "cell_type": "code",
      "source": [
        "#========================= XGBOOST REGRESSION =========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "# Loading the California housing dataset\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "X = data.data   # Features\n",
        "y = data.target # Target (housing prices)\n",
        "\n",
        "# Splitting into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=123\n",
        ")\n",
        "\n",
        "#--------------------------- METHOD 1: XGBRegressor with Decision Tree Base Learners ---------------------------\n",
        "# Initializing the XGBoost regressor\n",
        "xg_reg = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror', # Regression task\n",
        "    n_estimators=10,             # Number of boosting rounds\n",
        "    seed=123                     # Seed for reproducibility\n",
        ")\n",
        "\n",
        "# Fitting the model\n",
        "xg_reg.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions\n",
        "preds = xg_reg.predict(X_test)\n",
        "\n",
        "# Calculating RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "print(\"=\"*15 + \" XGBRegressor with Decision Tree Base Learners Results \" + \"=\"*15)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(\"=\"*85)\n",
        "\n",
        "#--------------------------- METHOD 2: XGBRegressor with Linear Base Learners ---------------------------\n",
        "# Converting data to DMatrix format for xgb.train\n",
        "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
        "DM_test = xgb.DMatrix(data=X_test, label=y_test)\n",
        "\n",
        "# Defining parameters for training\n",
        "params = {\n",
        "    \"booster\": \"gblinear\",          # Linear model as booster\n",
        "    \"objective\": \"reg:squarederror\" # Regression task\n",
        "}\n",
        "\n",
        "# Training the model using xgb.train\n",
        "xg_reg_dmatrix = xgb.train(\n",
        "    params=params, dtrain=DM_train, num_boost_round=10\n",
        ")\n",
        "\n",
        "# Making predictions\n",
        "preds_dmatrix = xg_reg_dmatrix.predict(DM_test)\n",
        "\n",
        "# Calculating RMSE\n",
        "rmse_dmatrix = np.sqrt(mean_squared_error(y_test, preds_dmatrix))\n",
        "print(\"\\n\" + \"=\"*15 + \" XGBRegressor with Linear Base Learners Results \" + \"=\"*22)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_dmatrix:.4f}\")\n",
        "print(\"=\"*85)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10e3f4b9-4cf2-4a94-9177-1e3b0b92f0fe",
        "outputId": "4a820aef-0206-44fa-de4b-cf07948a7751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============== XGBRegressor with Decision Tree Base Learners Results ===============\n",
            "Root Mean Squared Error (RMSE): 0.5378\n",
            "=====================================================================================\n",
            "\n",
            "=============== XGBRegressor with Linear Base Learners Results ======================\n",
            "Root Mean Squared Error (RMSE): 0.8951\n",
            "=====================================================================================\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "id": "daab3c36-11a6-453b-93cb-1b3d5905c2d8",
      "cell_type": "code",
      "source": [
        "#========================= XGBOOST REGRESSION L1 REGULARIZATION =========================\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "# Loading the California housing dataset\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "X = data.data   # Features\n",
        "y = data.target # Target (housing prices)\n",
        "\n",
        "# Converting data to an XGBoost DMatrix\n",
        "boston_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Setting base parameters for the model\n",
        "params = {\n",
        "    \"objective\": \"reg:squarederror\", # Regression task\n",
        "    \"max_depth\": 4                   # Maximum depth of the trees\n",
        "}\n",
        "\n",
        "# Defining different L1 regularization values (alpha)\n",
        "l1_params = [1, 10, 100]\n",
        "\n",
        "# List to store RMSE results for each alpha value\n",
        "rmses_l1 = []\n",
        "\n",
        "# Performing cross-validation for each L1 regularization value\n",
        "for reg in l1_params:\n",
        "    params[\"alpha\"] = reg  # Setting L1 regularization parameter (alpha)\n",
        "    cv_results = xgb.cv(\n",
        "        dtrain=boston_dmatrix,      # Data for training\n",
        "        params=params,              # Model parameters\n",
        "        nfold=4,                    # Number of cross-validation folds\n",
        "        num_boost_round=10,         # Number of boosting rounds\n",
        "        metrics=\"rmse\",             # Evaluation metric\n",
        "        as_pandas=True,             # Return results as a DataFrame\n",
        "        seed=123                    # Seed for reproducibility\n",
        "    )\n",
        "    # Extracting the final RMSE and appending to the results list\n",
        "    rmses_l1.append(cv_results[\"test-rmse-mean\"].iloc[-1])\n",
        "\n",
        "# Creating a DataFrame to display L1 values and corresponding RMSE\n",
        "results_df = pd.DataFrame(list(zip(l1_params, rmses_l1)), columns=[\"L1 (alpha)\", \"RMSE\"])\n",
        "\n",
        "# Displaying the results in a clean format\n",
        "print(\"=\"*30 + \" RESULTS \" + \"=\"*30)\n",
        "print(\"Best RMSE as a function of L1 (alpha):\")\n",
        "print(results_df)\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daab3c36-11a6-453b-93cb-1b3d5905c2d8",
        "outputId": "884e6f99-c8f5-4223-e5ff-bce74304c749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================== RESULTS ==============================\n",
            "Best RMSE as a function of L1 (alpha):\n",
            "   L1 (alpha)      RMSE\n",
            "0           1  0.588026\n",
            "1          10  0.590160\n",
            "2         100  0.626528\n",
            "======================================================================\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "id": "5738827c-6816-4597-85cc-6816bbfbd4d3",
      "cell_type": "code",
      "source": [
        "#==================== XGBOOST REGRESSION GRID AND RANDOMIZED SEARCH ====================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "# Loading the California housing dataset\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "X = data.data   # Features\n",
        "y = data.target # Target (housing prices)\n",
        "\n",
        "#--------------------------- GRID SEARCH CV ---------------------------\n",
        "print(\"=\"*30 + \" GRID SEARCH CV \" + \"=\"*30)\n",
        "\n",
        "# Defining parameter grid for GridSearchCV\n",
        "gbm_param_grid = {\n",
        "    \"learning_rate\": [0.01, 0.1, 0.5, 0.9], # Different learning rates\n",
        "    \"n_estimators\": [200],                 # Number of trees\n",
        "    \"subsample\": [0.3, 0.5, 0.9]           # Subsample ratios\n",
        "}\n",
        "\n",
        "# Initializing the XGBoost regressor\n",
        "gbm = xgb.XGBRegressor(objective=\"reg:squarederror\", seed=123)\n",
        "\n",
        "# Performing GridSearchCV\n",
        "grid_mse = GridSearchCV(\n",
        "    estimator=gbm,\n",
        "    param_grid=gbm_param_grid,\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    cv=4,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fitting the model to the data\n",
        "grid_mse.fit(X, y)\n",
        "\n",
        "# Extracting the best parameters and lowest RMSE\n",
        "print(\"Best Parameters Found:\", grid_mse.best_params_)\n",
        "print(\"Lowest RMSE Found:\", np.sqrt(np.abs(grid_mse.best_score_)))\n",
        "\n",
        "#--------------------------- RANDOMIZED SEARCH CV ---------------------------\n",
        "print(\"\\n\" + \"=\"*30 + \" RANDOMIZED SEARCH CV \" + \"=\"*24)\n",
        "\n",
        "# Defining parameter grid for RandomizedSearchCV\n",
        "gbm_param_dist = {\n",
        "    \"learning_rate\": np.arange(0.05, 1.05, 0.05), # Learning rates\n",
        "    \"n_estimators\": [200],                        # Number of trees\n",
        "    \"subsample\": np.arange(0.05, 1.05, 0.05)      # Subsample ratios\n",
        "}\n",
        "\n",
        "# Performing RandomizedSearchCV\n",
        "randomized_mse = RandomizedSearchCV(\n",
        "    estimator=gbm,\n",
        "    param_distributions=gbm_param_dist,\n",
        "    n_iter=25,  # Number of random combinations to test\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    cv=4,\n",
        "    verbose=1,\n",
        "    random_state=123\n",
        ")\n",
        "\n",
        "# Fitting the model to the data\n",
        "randomized_mse.fit(X, y)\n",
        "\n",
        "# Extracting the best parameters and lowest RMSE\n",
        "print(\"Best Parameters Found:\", randomized_mse.best_params_)\n",
        "print(\"Lowest RMSE Found:\", np.sqrt(np.abs(randomized_mse.best_score_)))"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5738827c-6816-4597-85cc-6816bbfbd4d3",
        "outputId": "7fd2192c-144f-4b5e-afda-af1395bacc2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================== GRID SEARCH CV ==============================\n",
            "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
            "Best Parameters Found: {'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.3}\n",
            "Lowest RMSE Found: 0.6587735420293971\n",
            "\n",
            "============================== RANDOMIZED SEARCH CV ========================\n",
            "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
            "Best Parameters Found: {'subsample': 0.5, 'n_estimators': 200, 'learning_rate': 0.05}\n",
            "Lowest RMSE Found: 0.6533834295714968\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "id": "9e178ea9-ca88-467d-8bbd-004a32143842",
      "cell_type": "code",
      "source": [
        "#==================== XGBOOST REGRESSION WITH PIPELINES ====================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
        "\n",
        "# Loading the California housing dataset\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "X = data.data   # Features\n",
        "y = data.target # Target (housing prices)\n",
        "\n",
        "# Creating a pipeline with StandardScaler and XGBoost regressor\n",
        "xgb_pipeline = Pipeline([\n",
        "    (\"st_scaler\", StandardScaler()),      # Standardize the features\n",
        "    (\"xgb_model\", xgb.XGBRegressor(seed=123)) # XGBoost regressor\n",
        "])\n",
        "\n",
        "# Performing cross-validation with RMSE as the evaluation metric\n",
        "scores = cross_val_score(\n",
        "    xgb_pipeline, X, y,\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    cv=10\n",
        ")\n",
        "\n",
        "# Calculating the final average RMSE\n",
        "final_avg_rmse = np.mean(np.sqrt(np.abs(scores)))\n",
        "\n",
        "# Displaying the results\n",
        "print(\"=\"*30 + \" CROSS-VALIDATION RESULTS \" + \"=\"*30)\n",
        "print(f\"Final Average RMSE (10-fold CV): {final_avg_rmse:.4f}\")\n",
        "print(\"=\"*86)\n",
        "\n",
        "#--------------------------- RANDOMIZED SEARCH CV FOR TUNING ---------------------------\n",
        "# Defining the parameter grid for hyperparameter tuning\n",
        "gbm_param_grid = {\n",
        "    'xgb_model__subsample': np.arange(0.05, 1, 0.05),\n",
        "    'xgb_model__max_depth': np.arange(3, 20, 1),\n",
        "    'xgb_model__colsample_bytree': np.arange(0.1, 1.05, 0.05)\n",
        "}\n",
        "\n",
        "# Performing RandomizedSearchCV to find the best parameters\n",
        "randomized_neg_mse = RandomizedSearchCV(\n",
        "    estimator=xgb_pipeline,          # Pipeline as the estimator\n",
        "    param_distributions=gbm_param_grid,\n",
        "    n_iter=10,                       # Number of random combinations to test\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    cv=4,                            # 4-fold cross-validation\n",
        "    verbose=0,                       # Verbosity for output\n",
        "    random_state=123                 # Reproducibility\n",
        ")\n",
        "\n",
        "# Fitting the model\n",
        "randomized_neg_mse.fit(X, y)\n",
        "\n",
        "# Calculating the best RMSE and displaying the best model\n",
        "best_rmse = np.sqrt(np.abs(randomized_neg_mse.best_score_))\n",
        "print(\"\\n\" + \"=\"*30 + \" RANDOMIZED SEARCH RESULTS \" + \"=\"*30)\n",
        "print(f\"Best RMSE: {best_rmse:.4f}\")\n",
        "print(\"Best Model Configuration:\")\n",
        "print(randomized_neg_mse.best_estimator_)\n",
        "print(\"=\"*86)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e178ea9-ca88-467d-8bbd-004a32143842",
        "outputId": "b56eca2b-e1f8-4291-b7ee-efa3e55c679b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================== CROSS-VALIDATION RESULTS ==============================\n",
            "Final Average RMSE (10-fold CV): 0.6172\n",
            "======================================================================================\n",
            "\n",
            "============================== RANDOMIZED SEARCH RESULTS ==============================\n",
            "Best RMSE: 0.6842\n",
            "Best Model Configuration:\n",
            "Pipeline(steps=[('st_scaler', StandardScaler()),\n",
            "                ('xgb_model',\n",
            "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
            "                              colsample_bylevel=None, colsample_bynode=None,\n",
            "                              colsample_bytree=0.6500000000000001, device=None,\n",
            "                              early_stopping_rounds=None,\n",
            "                              enable_categorical=False, eval_metric=None,\n",
            "                              feature_types=None, gamma=None, grow_policy=None,\n",
            "                              importance_type=None,\n",
            "                              interaction_constraints=None, learning_rate=None,\n",
            "                              max_bin=None, max_cat_threshold=None,\n",
            "                              max_cat_to_onehot=None, max_delta_step=None,\n",
            "                              max_depth=4, max_leaves=None,\n",
            "                              min_child_weight=None, missing=nan,\n",
            "                              monotone_constraints=None, multi_strategy=None,\n",
            "                              n_estimators=None, n_jobs=None,\n",
            "                              num_parallel_tree=None, random_state=None, ...))])\n",
            "======================================================================================\n"
          ]
        }
      ],
      "execution_count": 7
    }
  ]
}